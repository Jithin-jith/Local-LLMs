# 🧠 What is Ollama?

**Ollama** is a powerful **command-line tool and runtime** that makes it super easy to run and manage large language models (LLMs) locally on your machine.

> Think of it like **Docker**, but for **LLMs**.

Instead of worrying about downloading weights, converting formats, setting up tokenizers, or writing inference code, **Ollama wraps all of that into a single smooth experience**.

---

# ✅ What Can You Do with Ollama?

- 🏃‍♂️ **Run models** like **LLaMA 2**, **Mistral**, **Gemma**, **Phi**, **Codellama**, **OpenHermes**, **Neural Chat**, and more **locally**.
- 🧩 **Customize models** using `Modelfiles`. - https://github.com/ollama/ollama/blob/main/docs/modelfile.md
- 💬 **Chat** with models from your **terminal** or **build apps** using the **Ollama API**.
- 🛠️ **Fine-tune** or **personalize** models.
